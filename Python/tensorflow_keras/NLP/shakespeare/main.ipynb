{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZD9nYGqDs40"
      },
      "source": [
        "# Génération de texte avec les réseaux de neurones\n",
        "\n",
        "Dans ce notebook, nous allons créer un réseau qui peut générer du texte, ici nous montrons que cela se fait caractère par caractère. Super post à lire ici : http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "Nous avons organisé le processus en \"étapes\" afin que vous puissiez suivre facilement avec vos propres ensembles de données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SU2Pb4meDs45"
      },
      "source": [
        "## Étape 1 : Les Données\n",
        "\n",
        "Vous pouvez récupérer n'importe quel texte gratuit ici : https://www.gutenberg.org/\n",
        "\n",
        "Nous allons choisir toutes les œuvres de Shakespeare (que nous avons déjà téléchargées pour vous), principalement pour deux raisons :\n",
        "\n",
        "1. C'est un grand corpus de textes, il est généralement recommandé d'avoir au moins une source d'un million de caractères au total pour obtenir une génération de texte réaliste.\n",
        "\n",
        "2. Il a un style très particulier. Comme les données textuelles utilisent un anglais ancien et sont formatées dans le style d'une pièce de théâtre, il nous apparaîtra très clairement si le modèle est capable de reproduire des résultats similaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U_nzKqC_Ds46"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x9atN0FjDs47",
        "outputId": "d00f3e12-b8eb-4b26-d180-8eed2f664ec2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  But thou contracted to thine own bright eyes,\n",
            "  Feed'st thy light's flame with self-substantial fuel,\n",
            "  Making a famine where abundance lies,\n",
            "  Thy self thy foe, to thy sweet self too cruel:\n",
            "  Thou that art now the world's fresh ornament,\n",
            "  And only herald to the gaudy spring,\n",
            "  Within thine own bu\n"
          ]
        }
      ],
      "source": [
        "text = open(\"shakespeare.txt\", 'r').read()\n",
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PgLeWKkrDs48"
      },
      "source": [
        "### Comprendre les caractères uniques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXZ0lD5FDs48",
        "outputId": "70c840ea-1492-4793-e083-b0e2eaf5ff5e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['\\n', ' ', '!', '\"', '&', \"'\", '(', ')', ',', '-', '.', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '>', '?', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', ']', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '|', '}']\n",
            "84\n"
          ]
        }
      ],
      "source": [
        "vocab = sorted(set(text))\n",
        "print(vocab)\n",
        "print(len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3frw8CNDs48"
      },
      "source": [
        "## Étape 2 : Traitement du texte\n",
        "\n",
        "### Vectorisation du texte\n",
        "\n",
        "Nous savons qu'un réseau de neurones ne peut pas prendre en charge les données brutes des chaînes de caractères, nous devons attribuer des numéros à chaque caractère. Créons deux dictionnaires qui peuvent passer d'un index numérique à un caractère et d'un caractère à un index numérique."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "05qPlDqyDs48",
        "outputId": "2a390f86-a9bb-44cc-dca4-a351a9944fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'\\n': 0, ' ': 1, '!': 2, '\"': 3, '&': 4, \"'\": 5, '(': 6, ')': 7, ',': 8, '-': 9}\n",
            "2\n",
            "['\\n' ' ' '!' '\"' '&']\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "char_to_ind = {u:i for i, u in enumerate(vocab)}\n",
        "print({k: char_to_ind[k] for k in list(char_to_ind)[:10]})\n",
        "print(char_to_ind[\"!\"])\n",
        "ind_to_char = np.array(vocab)\n",
        "print(ind_to_char[:5])\n",
        "ind_to_char[2]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4yA-vM4TDs49",
        "outputId": "e9a48bc1-e0f1-4fb5-a0cc-db5e1c5085d6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But as the riper should by time decease,\n",
            "  His tender heir might bear his memory:\n",
            "  => [ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
            "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
            "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
            " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
            " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75\n",
            "  1 56 74  1 75 63 60  1 73 64 71 60 73  1 74 63 70 76 67 59  1 57 80  1\n",
            " 75 64 68 60  1 59 60 58 60 56 74 60  8  0  1  1 33 64 74  1 75 60 69 59\n",
            " 60 73  1 63 60 64 73  1 68 64 62 63 75  1 57 60 56 73  1 63 64 74  1 68\n",
            " 60 68 70 73 80 21  0  1]\n",
            "\n",
            "(5445609,)\n"
          ]
        }
      ],
      "source": [
        "# Cela convertit le texte en nombres entiers\n",
        "encoded_text = np.array([char_to_ind[c] for c in text])\n",
        "print(f'{text[:200]} => {encoded_text[:200]}')\n",
        "print()\n",
        "print(encoded_text.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MhS2uiXGDs49"
      },
      "source": [
        "## Étape 3 : Création de batches\n",
        "\n",
        "Globalement, ce que nous essayons de faire, c'est de faire en sorte que le modèle prévoie le caractère suivant le plus probable, compte tenu d'une séquence historique de caractères. C'est à nous (l'utilisateur) de choisir la longueur de cette séquence historique. Une séquence trop courte et nous n'aurons pas assez d'informations (par exemple, étant donné la lettre \"a\", quel est le prochain caractère), une séquence trop longue et l'entraînement prendra trop de temps et risque de sur-entraîner (au risque d'obtenir une séquence de caractères qui ne sont pas pertinents pour des caractères plus éloignés). Bien qu'il n'y ait pas de choix correct de longueur de séquence, vous devez considérer le texte lui-même, la longueur des phrases normales qu'il contient et avoir une idée raisonnable des caractères/mots qui sont pertinents les uns pour les autres."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANHMSkS2Ds49",
        "outputId": "087811f4-eea1-40c0-e456-f9439cdb7895"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Le nombre moyen de caractères pour 3 lignes est 43.67 | total de caractères: 131\n"
          ]
        }
      ],
      "source": [
        "def average_chars_per_line(text, start_line, end_line):\n",
        "    lines = text.split('\\n')[start_line-1:end_line]\n",
        "    total_chars = sum(len(line) for line in lines)\n",
        "    average_chars = total_chars / len(lines)\n",
        "    return lines, total_chars, average_chars\n",
        "\n",
        "lines, total_chars, average_chars = average_chars_per_line(text, 3, 5)\n",
        "print(f'Le nombre moyen de caractères pour {len(lines)} lignes est {average_chars:.2f} | total de caractères: {total_chars}')\n",
        "# Nous avons un total de caractères de 131 pour 3 lignes, on peut donc choisir une longueur de séquence de 120 caractères\n",
        "# pour avoir assez d'informations pour capturer l'essence du texte et pas trop grand pour ne pas être trop lents pendant l'entraînement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-vyWDwwbDs4-"
      },
      "source": [
        "### Séquences d'Entraînement\n",
        "\n",
        "Les données textuelles réelles seront la séquence de texte décalée d'un caractère vers l'avant.\n",
        "\n",
        "Par exemple :\n",
        "\n",
        "*   Sequence In: \"Hello my nam\"\n",
        "*   Sequence Out: \"ello my name\"\n",
        "\n",
        "Nous pouvons utiliser la fonction `tf.data.Dataset.from_tensor_slices` pour convertir un vecteur de texte en un flux d'indices de caractères."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JBaJsikUDs4-",
        "outputId": "ab138f9a-f3bf-4360-88b4-5b8d515c8e5d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nombre total de séquences: 45005\n"
          ]
        }
      ],
      "source": [
        "seq_len = 120\n",
        "total_num_seq = len(text) // (seq_len + 1)\n",
        "print(\"Nombre total de séquences:\", total_num_seq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FmuH3Z-PDs4-"
      },
      "outputs": [],
      "source": [
        "# Crée un dataset à partir du texte encodé\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(encoded_text)\n",
        "\n",
        "# Divise le dataset en séquences de longueur seq_len + 1\n",
        "# drop_remainder=True permet de supprimer les séquences qui ne sont pas de la longueur souhaitée\n",
        "sequences = char_dataset.batch(seq_len + 1, drop_remainder=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3cwhVy9kDs4-"
      },
      "source": [
        "Maintenant que nous avons nos séquences, nous allons effectuer les étapes suivantes pour chacune d'entre elles afin de créer nos séquences de texte cible :\n",
        "\n",
        "1. Saisir la séquence de texte d'entrée\n",
        "2. Assigner la séquence de texte cible comme séquence de texte d'entrée décalée d'un pas en avant\n",
        "3. Regroupez-les en un tuple"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNdt7w49Ds4_"
      },
      "outputs": [],
      "source": [
        "def create_seq_targets(seq):\n",
        "    input_txt = seq[:-1] # Hello my nam\n",
        "    target_txt = seq[1:] # ello my name\n",
        "    return input_txt, target_txt # (Hello my nam, ello my name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bBQPYE3sDs4_",
        "outputId": "987a8927-b000-4ddd-b073-a2b55943522f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ 0  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0\n",
            "  1  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74\n",
            "  1 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45\n",
            " 63 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74\n",
            " 60  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75]\n",
            "\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But\n",
            "\n",
            "\n",
            "[ 1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1  1 12  0  1\n",
            "  1 31 73 70 68  1 61 56 64 73 60 74 75  1 58 73 60 56 75 76 73 60 74  1\n",
            " 78 60  1 59 60 74 64 73 60  1 64 69 58 73 60 56 74 60  8  0  1  1 45 63\n",
            " 56 75  1 75 63 60 73 60 57 80  1 57 60 56 76 75 80  5 74  1 73 70 74 60\n",
            "  1 68 64 62 63 75  1 69 60 77 60 73  1 59 64 60  8  0  1  1 27 76 75  1]\n",
            "                     1\n",
            "  From fairest creatures we desire increase,\n",
            "  That thereby beauty's rose might never die,\n",
            "  But \n"
          ]
        }
      ],
      "source": [
        "dataset = sequences.map(create_seq_targets)\n",
        "for input_txt, target_txt in dataset.take(1):\n",
        "    print(input_txt.numpy())\n",
        "    print(\"\".join(ind_to_char[input_txt.numpy()]))\n",
        "    print('\\n')\n",
        "    print(target_txt.numpy())\n",
        "    print(\"\".join(ind_to_char[target_txt.numpy()]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIT9hgPwDs4_"
      },
      "source": [
        "### Générer des batches d'entraînement\n",
        "\n",
        "Maintenant que nous avons les séquences réelles, nous allons créer les lots, nous voulons mélanger ces séquences dans un ordre aléatoire, de sorte que le modèle ne s'adapte à aucune section du texte, mais puisse au contraire générer des caractères à partir de n'importe quel texte de départ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuzsMoiXDs4_",
        "outputId": "3b634378-35a8-498d-9933-c7f8a6f31e7c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_BatchDataset element_spec=(TensorSpec(shape=(128, 120), dtype=tf.int64, name=None), TensorSpec(shape=(128, 120), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "batch_size = 128 # Nombre de séquences dans chaque batch\n",
        "buffer_size = 10000 # Nous prenons 10000 séquences pour les mélanger aléatoirement\n",
        "dataset = dataset.shuffle(buffer_size).batch(batch_size, drop_remainder=True)\n",
        "dataset\n",
        "# ici chaque batch contient 128 séquences, chaque séquence contient 120 caractères et chaque caractère est représenté par un nombre entier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CBL8CbQXDs4_"
      },
      "source": [
        "## Étape 4 : Création du modèle\n",
        "\n",
        "Nous utiliserons un modèle basé sur le LSTM avec quelques caractéristiques supplémentaires, notamment une couche embedding pour commencer et **deux** couches LSTM. Nous avons basé cette architecture de modèle sur le [DeepMoji](https://deepmoji.mit.edu/) et le code source original peut être trouvé [ici](https://github.com/bfelbo/DeepMoji).\n",
        "\n",
        "La couche embedding servira de couche d'entrée, qui crée essentiellement une table de consultation qui fait correspondre les indices numériques de chaque caractère à un vecteur avec un nombre de dimensions \"embedding dim\". Comme vous pouvez l'imaginer, plus cette taille d'embedding est grande, plus l'entraînement est complexe. C'est similaire à l'idée derrière word2vec, où les mots sont mis en correspondance avec un espace à n dimensions. L'embedding avant le feeding directe dans le LSTM conduit généralement à des résultats plus réalistes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6D8ENQNkDs4_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.losses import sparse_categorical_crossentropy\n",
        "\n",
        "def sparse_cat_loss(y_true, y_pred):\n",
        "    return sparse_categorical_crossentropy(y_true, y_pred, from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lQWi3NxYDs4_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, GRU, Dense"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZwc39dUDs4_"
      },
      "outputs": [],
      "source": [
        "# Longueur du vocabulaire en caractères\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# La dimension embedding\n",
        "embed_dim = 64\n",
        "\n",
        "# Nombre d'unitées RNN\n",
        "rnn_neurons = 1026\n",
        "\n",
        "def create_model(vocab_size, embed_dim, rnn_neurons, batch_size):\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(vocab_size, embed_dim, batch_input_shape=[batch_size, None]))\n",
        "    model.add(GRU(rnn_neurons, return_sequences=True, stateful=True, recurrent_initializer='glorot_uniform'))\n",
        "    model.add(Dense(vocab_size))\n",
        "    model.compile(optimizer='adam', loss=sparse_cat_loss)\n",
        "    model.summary()\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FKE7rrTzDs5A",
        "outputId": "3112ac07-4b51-4c2b-b4c0-43b63937234e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (128, None, 64)           5376      \n",
            "                                                                 \n",
            " gru (GRU)                   (128, None, 1026)         3361176   \n",
            "                                                                 \n",
            " dense (Dense)               (128, None, 84)           86268     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3452820 (13.17 MB)\n",
            "Trainable params: 3452820 (13.17 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model = create_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embed_dim=embed_dim,\n",
        "  rnn_neurons=rnn_neurons,\n",
        "  batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWxeQ992Ds5A"
      },
      "source": [
        "## Étape 5 : Entraînement du modèle\n",
        "\n",
        "Assurons-nous que tout va bien avec notre modèle avant de passer trop de temps sur l'entraînement ! Passons en lot pour confirmer que le modèle prédit actuellement des caractères aléatoires sans aucun entraînement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-u8Pd5pvDs5A",
        "outputId": "ba53661d-98d6-4b23-fbc5-2de9570ffcb2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(128, 120, 84) => (batch_size, sequence_length, vocab_size)\n",
            "[72  3 52 61 25 23 15  4 56 56 53 81 24 23 54 76  5 57 58 50 11 43 40 34\n",
            " 63 22 58 28 60 23 27 42  5 54 78 14 67 56  2 78 23 11  9 69 57 80 15 15\n",
            " 27 62 60 50 68 13 19 76 11 51 20 52 82 65 60  1 17 31 65 29 28 77 70  6\n",
            " 16  2 49 80 41  4 36 29 47 76 66 58 69 67  4 48  3 64  5 42 48 42 12 82\n",
            " 34  7 63 44 35 60  9 63  9 29 19 19 60 68 25 72 41  6 22 71 72 14 48 66]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['q', '\"', '[', 'f', '?', '<', '4', '&', 'a', 'a', ']', 'z', '>',\n",
              "       '<', '_', 'u', \"'\", 'b', 'c', 'Y', '0', 'R', 'O', 'I', 'h', ';',\n",
              "       'c', 'C', 'e', '<', 'B', 'Q', \"'\", '_', 'w', '3', 'l', 'a', '!',\n",
              "       'w', '<', '0', '-', 'n', 'b', 'y', '4', '4', 'B', 'g', 'e', 'Y',\n",
              "       'm', '2', '8', 'u', '0', 'Z', '9', '[', '|', 'j', 'e', ' ', '6',\n",
              "       'F', 'j', 'D', 'C', 'v', 'o', '(', '5', '!', 'X', 'y', 'P', '&',\n",
              "       'K', 'D', 'V', 'u', 'k', 'c', 'n', 'l', '&', 'W', '\"', 'i', \"'\",\n",
              "       'Q', 'W', 'Q', '1', '|', 'I', ')', 'h', 'S', 'J', 'e', '-', 'h',\n",
              "       '-', 'D', '8', '8', 'e', 'm', '?', 'q', 'P', '(', ';', 'p', 'q',\n",
              "       '3', 'W', 'k'], dtype='<U1')"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1): # 1 élément du dataset\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"=> (batch_size, sequence_length, vocab_size)\")\n",
        "\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1) # transforme les probabilités en nombres entiers (indices)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy() # supprime la dimension inutile\n",
        "print(sampled_indices)\n",
        "ind_to_char[sampled_indices] # convertit les nombres entiers en caractères"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hvW0xLwRDs5A",
        "outputId": "3ac3f37a-5b69-4b92-9230-9334b254a303"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "351/351 [==============================] - 50s 125ms/step - loss: 2.5675\n",
            "Epoch 2/30\n",
            "351/351 [==============================] - 45s 123ms/step - loss: 1.7607\n",
            "Epoch 3/30\n",
            "351/351 [==============================] - 45s 123ms/step - loss: 1.4809\n",
            "Epoch 4/30\n",
            "351/351 [==============================] - 46s 123ms/step - loss: 1.3541\n",
            "Epoch 5/30\n",
            "351/351 [==============================] - 46s 127ms/step - loss: 1.2879\n",
            "Epoch 6/30\n",
            "351/351 [==============================] - 48s 131ms/step - loss: 1.2453\n",
            "Epoch 7/30\n",
            "351/351 [==============================] - 47s 126ms/step - loss: 1.2144\n",
            "Epoch 8/30\n",
            "351/351 [==============================] - 47s 130ms/step - loss: 1.1901\n",
            "Epoch 9/30\n",
            "351/351 [==============================] - 48s 133ms/step - loss: 1.1695\n",
            "Epoch 10/30\n",
            "351/351 [==============================] - 51s 135ms/step - loss: 1.1517\n",
            "Epoch 11/30\n",
            "351/351 [==============================] - 47s 127ms/step - loss: 1.1364\n",
            "Epoch 12/30\n",
            "351/351 [==============================] - 48s 131ms/step - loss: 1.1220\n",
            "Epoch 13/30\n",
            "351/351 [==============================] - 48s 132ms/step - loss: 1.1083\n",
            "Epoch 14/30\n",
            "351/351 [==============================] - 50s 138ms/step - loss: 1.0956\n",
            "Epoch 15/30\n",
            "351/351 [==============================] - 50s 136ms/step - loss: 1.0838\n",
            "Epoch 16/30\n",
            "351/351 [==============================] - 50s 137ms/step - loss: 1.0724\n",
            "Epoch 17/30\n",
            "351/351 [==============================] - 51s 138ms/step - loss: 1.0615\n",
            "Epoch 18/30\n",
            "351/351 [==============================] - 50s 135ms/step - loss: 1.0511\n",
            "Epoch 19/30\n",
            "351/351 [==============================] - 51s 137ms/step - loss: 1.0412\n",
            "Epoch 20/30\n",
            "351/351 [==============================] - 51s 136ms/step - loss: 1.0322\n",
            "Epoch 21/30\n",
            "351/351 [==============================] - 50s 137ms/step - loss: 1.0237\n",
            "Epoch 22/30\n",
            "351/351 [==============================] - 50s 137ms/step - loss: 1.0155\n",
            "Epoch 23/30\n",
            "351/351 [==============================] - 50s 137ms/step - loss: 1.0080\n",
            "Epoch 24/30\n",
            "351/351 [==============================] - 50s 137ms/step - loss: 1.0016\n",
            "Epoch 25/30\n",
            "351/351 [==============================] - 47s 130ms/step - loss: 0.9958\n",
            "Epoch 26/30\n",
            "351/351 [==============================] - 50s 133ms/step - loss: 0.9907\n",
            "Epoch 27/30\n",
            "351/351 [==============================] - 47s 128ms/step - loss: 0.9855\n",
            "Epoch 28/30\n",
            "351/351 [==============================] - 48s 132ms/step - loss: 0.9809\n",
            "Epoch 29/30\n",
            "351/351 [==============================] - 50s 135ms/step - loss: 0.9768\n",
            "Epoch 30/30\n",
            "351/351 [==============================] - 47s 127ms/step - loss: 0.9738\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7d304081e950>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "epochs = 30\n",
        "model.fit(dataset, epochs=epochs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KgbnUUJVDs5A"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "model.save(\"model.keras\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Étape 6 : Génération du texte\n",
        "\n",
        "Actuellement, notre modèle ne prévoit que 128 séquences à la fois. Nous pouvons créer un nouveau modèle qui n'attend qu'un batch_size=1. Nous pouvons créer un nouveau modèle avec cette taille de batch, puis charger les poids de nos modèles sauvegardés. Ensuite, appelez .build() sur le modèle :"
      ],
      "metadata": {
        "id": "mJo57Zx0F-rb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model"
      ],
      "metadata": {
        "id": "sncyo5ctFySA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(vocab_size, embed_dim, rnn_neurons, batch_size=1) # batch_size=1 pour que le modèle puisse accepter une séquence de n'importe quelle longueur\n",
        "model.load_weights(\"model.keras\") # charger les poids du modèle entraîné\n",
        "model.build(tf.TensorShape([1, None])) # construire le modèle avec une longueur de séquence de 1\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O_6TCTsCMoYE",
        "outputId": "f76fe706-6c76-4b93-8427-b0ef60b8aa3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (1, None, 64)             5376      \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (1, None, 1026)           3361176   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (1, None, 84)             86268     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3452820 (13.17 MB)\n",
            "Trainable params: 3452820 (13.17 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Skipping variable loading for optimizer 'Adam', because it has 1 variables whereas the saved optimizer has 13 variables. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (1, None, 64)             5376      \n",
            "                                                                 \n",
            " gru_1 (GRU)                 (1, None, 1026)           3361176   \n",
            "                                                                 \n",
            " dense_1 (Dense)             (1, None, 84)             86268     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 3452820 (13.17 MB)\n",
            "Trainable params: 3452820 (13.17 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\"\"\"\n",
        "Génère du texte en utilisant un modèle donné et une chaîne de départ.\n",
        "\n",
        "Paramètres:\n",
        "- model: Le modèle entraîné utilisé pour la génération de texte.\n",
        "- start_string: La chaîne initiale pour démarrer le processus de génération.\n",
        "- num_generate: Le nombre de caractères à générer (par défaut: 1000).\n",
        "- temperature: La valeur de température pour contrôler l'aléatoire du texte généré (par défaut: 1.0).\n",
        "\n",
        "Retourne:\n",
        "- Le texte généré en tant que chaîne.\n",
        "\"\"\"\n",
        "def generate_text(model, start_string, num_generate=1000, temperature=1.0):\n",
        "    # Convertit la chaîne de départ en nombres (vectorisation)\n",
        "    input_eval = [char_to_ind[s] for s in start_string]\n",
        "    input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "    # Chaîne vide pour stocker le texte généré\n",
        "    generated_text = []\n",
        "\n",
        "    # Abaisse la température pour rendre la sortie plus aléatoire\n",
        "    model.reset_states() # prendre en compte le taille de batch_size à 1\n",
        "    for i in range(num_generate):\n",
        "        predictions = model(input_eval)\n",
        "        predictions = tf.squeeze(predictions, 0) # retirer la dimension faite par expand_dims\n",
        "\n",
        "        # Apply temperature to the predictions\n",
        "        predictions = predictions / temperature\n",
        "        predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
        "\n",
        "        # Passe le caractère prédit comme prochaine entrée du modèle\n",
        "        input_eval = tf.expand_dims([predicted_id], 0)\n",
        "        generated_text.append(ind_to_char[predicted_id])\n",
        "\n",
        "    return start_string + ''.join(generated_text)"
      ],
      "metadata": {
        "id": "keHQhFOCMuRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(generate_text(model=model, start_string=\"Romeo\", num_generate=500))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zTdy7H0JM5Pd",
        "outputId": "bdd4d5be-22e2-4f7c-ed1c-ae015d74ac60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Romeove, I am s\n",
            "  PETOR. Most gracious Prince! think you all 'tis a fair return.\n",
            "    Yet, it grows a that thus have sport alive,\n",
            "    And say 'The man I'll lay their vows, haply, my lord;\n",
            "    The cleare is to this paper for the girn\n",
            "    Had not it for thy print you come, and I will pay.\n",
            "    How cam'd your counsel when I am a cut,\n",
            "    It was my reapon, I\n",
            "    Hast discovered horns, a practice for\n",
            "    A prayer and twenty years and bad a breach:\n",
            "    I answer in my title to the heavens\n",
            "    Give to my wealt\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}