{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Les RNN sont particulièrement adaptés au traitement de données séquentielles, où l'ordre des éléments détient une information significative. L'objectif est de prédire le future. Les exemples de données séquentielles incluent des séries temporelles, des phrases, des données audio, des trajectoires de véhicules, etc. L'ordre des éléments est crucial pour comprendre l'information dans de telles données.\n",
    "\n",
    "<img src=\"rnn-example-1.jpg\" width=50%>\n",
    "\n",
    "<img src=\"rnn-example-2.jpg\" width=50%>\n",
    "\n",
    "### Prédiction des Décalages Séquentiels\n",
    "\n",
    "## Neurones Récursifs\n",
    "\n",
    "La principale différence entre un neurone normal et un neurone récurrent réside dans la capacité du neurone récurrent à maintenir une mémoire des entrées précédentes. Les neurones récurrents sont conçus pour traiter des données séquentielles en introduisant un mécanisme de rétroaction qui permet à la sortie d'un pas de temps précédent de devenir une entrée pour le pas de temps suivant.\n",
    "\n",
    "Neurone Normal (Feedforward Neuron) :\n",
    "\n",
    "- Prend un ensemble d'entrées et effectue une opération d'agrégation.\n",
    "- Applique une fonction d'activation pour produire une sortie.\n",
    "- Ne garde pas de mémoire des sorties passées.\n",
    "- Convient généralement pour des tâches où l'ordre des données n'est pas significatif.\n",
    "\n",
    "<img src=\"normal-neuron.jpg\" width=50%>\n",
    "\n",
    "<img src=\"normal-neurons.jpg\" width=50%>\n",
    "\n",
    "\n",
    "Neurone Récurrent :\n",
    "\n",
    "- Prend un ensemble d'entrées, mais ajoute également une entrée provenant de la sortie à l'étape précédente.\n",
    "- Utilise une fonction d'activation pour produire une sortie.\n",
    "- La sortie précédente est utilisée pour rétroagir dans le temps et influencer la sortie actuelle.\n",
    "- Peut conserver une mémoire à court terme des informations séquentielles.\n",
    "- Convient pour des tâches où l'ordre des données est crucial, comme dans les séquences temporelles ou le langage naturel.\n",
    "\n",
    "En résumé, les neurones récurrents introduisent une dimension temporelle, permettant de traiter des données séquentielles en conservant une mémoire des états précédents. Cela les rend adaptés à une variété de tâches liées à la séquence, telles que la prédiction temporelle, la génération de texte et la compréhension du langage naturel.\n",
    "\n",
    "<img src=\"rnn-neuron.jpg\" width=50%>\n",
    "\n",
    "<img src=\"normal-neuron-2.jpg\" width=50%>\n",
    "\n",
    "<img src=\"rnn-neurons.jpg\" width=50%>\n",
    "\n",
    "Plusieurs neurones récurrents peuvent être organisés en une couche RNN. Cette couche est déroulée dans le temps, permettant à chaque neurone de conserver une mémoire de sortie.\n",
    "\n",
    "<img src=\"rnn-neurons-2.jpg\" width=50%>\n",
    "\n",
    "## Type architectures des RNN\n",
    "\n",
    "Many-to-Many (Sequence-to-Sequence) :\n",
    "\n",
    "- Entrée : Une séquence d'informations.\n",
    "- Sortie : Une séquence d'informations.\n",
    "- Exemple : Prédire les mots suivants dans une phrase.\n",
    "\n",
    "Many-to-One :\n",
    "\n",
    "- Entrée : Une séquence d'informations.\n",
    "- Sortie : Une seule sortie.\n",
    "- Exemple : Prédire le prochain mot dans une phrase.\n",
    "\n",
    "One-to-Many :\n",
    "\n",
    "- Entrée : Une seule entrée.\n",
    "- Sortie : Une séquence d'informations.\n",
    "- Exemple : Générer une légende pour une image.\n",
    "\n",
    "One-to-One (Feedforward) :\n",
    "\n",
    "- Entrée : Une seule entrée.\n",
    "- Sortie : Une seule sortie.\n",
    "- Exemple : Classification d'image unique.\n",
    "\n",
    "## Défis des RNN de Base\n",
    "\n",
    "Les RNN de base ont des limitations. Ils dépendent principalement de la mémoire à court terme et peuvent oublier des informations historiques plus anciennes. De plus, le problème de la disparition du gradient peut entraver l'apprentissage.\n",
    "\n",
    "## Problème de la Disparition du Gradient\n",
    "\n",
    "Pour résoudre ce problème nous approfondirons le problème de la disparition du gradient avant d'introduire les unités de mémoire à court et long terme (LSTM) comme solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les unités LSTM (Long Short Term Memory) et GRU (Gated Recurrent Unit) sont des éléments essentiels des réseaux neuronaux récurrents (RNN) qui aident à résoudre des problèmes spécifiques liés à l'apprentissage sur des séquences temporelles.\n",
    "\n",
    "## Problèmes avec les RNN Traditionnels\n",
    "\n",
    "Les RNN traditionnels peuvent avoir des difficultés à conserver des informations sur de longues séquences. Par exemple, ils peuvent oublier des éléments importants lors de la rétropropagation du gradient pendant l'entraînement.\n",
    "\n",
    "## Solution : Les Unités LSTM\n",
    "\n",
    "Les unités LSTM ont été conçues pour résoudre ces problèmes en introduisant une mémoire à court terme et à long terme. Elles sont composées de différentes \"portes\" qui permettent de contrôler le flux d'informations à travers le réseau.\n",
    "\n",
    "## Composants Principaux d'une Unité LSTM\n",
    "\n",
    "<img src=\"ltsm.jpg\" width=\"35%\">\n",
    "\n",
    "<img src=\"ltsm-2.jpg\" width=\"30.5%\">\n",
    "\n",
    "1. **Porte d'Oubli (Forget Gate):**  décide quelles informations de l'état cellulaire précédent (mémoire à long terme) doivent être oubliées ou conservées. La fonction sigmoïde est utilisée pour produire des valeurs entre 0 et 1, où 0 signifie \"oublier complètement\" et 1 signifie \"se souvenir intégralement\". Cela permet à l'unité LSTM de gérer les informations importantes sur de longues séquences temporelles.\n",
    "2. **Porte d'Entrée (Input Gate):** détermine quelles nouvelles informations de l'entrée actuelle doivent être ajoutées à l'état cellulaire. La fonction sigmoïde est utilisée pour réguler quelles parties de l'entrée doivent être mises à jour. Une autre fonction, généralement la tangente hyperbolique (tanh), crée un vecteur de nouvelles informations potentielles qui pourraient être ajoutées à l'état cellulaire.\n",
    "3. **Porte de Mise à Jour (Update Gate):** combine l'ancien état cellulaire avec les nouvelles informations pondérées décidées par la porte d'entrée. Cela met à jour l'état cellulaire (mémoire à long terme) en tenant compte des informations importantes de l'entrée actuelle.\n",
    "4. **Porte de Sortie (Output Gate):**  filtre l'état cellulaire mis à jour pour produire la sortie finale. La sortie est régulée en fonction de l'état cellulaire et de l'entrée actuelle. Cela permet de générer une sortie utile en tenant compte à la fois de la mémoire à court terme et de la nouvelle information.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
